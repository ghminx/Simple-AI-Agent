{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import List, Dict, Callable\n",
    "from utils import llm_call\n",
    "import re \n",
    "import os \n",
    "from openai import OpenAI\n",
    "from groq import Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input1 = '저녁메뉴 1개만 추천해줘'\n",
    "\n",
    "user_input2 = \"ChatGPT와 일반 챗봇의 차이점을 간단히 설명해줘.\"\n",
    "\n",
    "user_input3 = \"OverFitting의 개념을 알려주고 해결 방법에는 어떤 것들이 있는지 구체적인 예시를 들어서 상세히 알려줘\"\n",
    "\n",
    "\n",
    "def route(user_input : str) -> str:\n",
    "    select_prompt = f'''\n",
    "\n",
    "    user_input : {user_input}\n",
    "\n",
    "\n",
    "    당신은 AI 라우팅 시스템입니다.\n",
    "    사용자의 요청을 보고 가장 적절한 AI 모델을 선택하세요.\n",
    "\n",
    "    선택 가능한 모델:\n",
    "    - gemma2-9b-it (요청이 매우 간단하거나 반복적인 경우)\n",
    "    - llama3-70b-8192 (일반적인 질문이나 가벼운 생성 작업)\n",
    "    - deepseek-r1-distill-llama-70b-specdec (복잡하거나 창의적 사고가 필요한 고급 요청)\n",
    "\n",
    "\n",
    "    - 사용자의 질문에는 답변하지 않습니다.\n",
    "    - 먼저 왜 해당 모델을 선택했는지 간단히 설명하고, 아래 형식으로 **반드시 그대로 출력**해주세요:\n",
    "\n",
    "\n",
    "    출력 예시:\n",
    "\n",
    "    선택 이유: 사용자의 요청이 어떤 유형이며, 어떤 모델이 그에 적합한지 설명.\n",
    "    선택한 모델 이름 : \"gpt-4o-mini\"\n",
    "\n",
    "    '''\n",
    "\n",
    "    select = llm_call(select_prompt)\n",
    "    \n",
    "    reason = select.split('\\n')[0]\n",
    "    \n",
    "    if not select:\n",
    "        raise \"No model selected\"\n",
    "    \n",
    "    model_name = re.search(r'\\\"(.*?)\\\"', select).group(1)\n",
    "    \n",
    "    print(f'Reasoning Select Model : {reason}')\n",
    "    print(f'selected model : {model_name}')\n",
    "    \n",
    "    response = llm_call(user_input, model_name)\n",
    "    \n",
    "    return response\n",
    "\n",
    "route(user_input3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Reasoning Select Model : {reason})\n",
    "      \n",
    "\n",
    "      \n",
    "print(f'selected model : {model_name})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_prompt = f'''\n",
    "\n",
    "user_input : {user_input1}\n",
    "\n",
    "\n",
    "당신은 AI 라우팅 시스템입니다.\n",
    "사용자의 요청을 보고 가장 적절한 AI 모델을 선택하세요.\n",
    "\n",
    "선택 가능한 모델:\n",
    "- gemma2-9b-it (요청이 매우 간단하거나 반복적인 경우)\n",
    "- llama3-70b-8192 (일반적인 질문이나 가벼운 생성 작업)\n",
    "- deepseek-r1-distill-llama-70b-specdec (복잡하거나 창의적 사고가 필요한 고급 요청)\n",
    "\n",
    "\n",
    "- 사용자의 질문에는 답변하지 않습니다.\n",
    "- 먼저 왜 해당 모델을 선택했는지 간단히 설명하고, 그 다음 아래 처럼 출력해주세요\n",
    "\n",
    "\n",
    "출력 예시:\n",
    "\n",
    "선택 이유: 사용자의 요청이 어떤 유형이며, 어떤 모델이 그에 적합한지 설명.\n",
    "선택한 모델 이름 : \"gpt-4o-mini\"\n",
    "\n",
    "'''\n",
    "\n",
    "select = llm_call(select_prompt)\n",
    "\n",
    "print(select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from utils import llm_call\n",
    "\n",
    "prompt = \"다음 문장을 영어로 번역해주세요\"\n",
    "inputs = [\n",
    "    \"ChatGPT는 OpenAI가 개발한 대형 언어 모델입니다.\",\n",
    "    \"AI 기술은 산업 전반에 혁신을 불러오고 있습니다.\",\n",
    "    \"기후 변화는 글로벌 이슈 중 하나입니다.\"\n",
    "]\n",
    "\n",
    "def parallel(prompt: str, inputs: List[str], n_workers: int = 3) -> List[str]:\n",
    "    \"\"\"Process multiple inputs concurrently with the same prompt.\"\"\"\n",
    "    with ThreadPoolExecutor(max_workers=n_workers) as executor:\n",
    "        futures = [executor.submit(llm_call, f\"{prompt}\\nInput: {x}\") for x in inputs]\n",
    "        return [f.result() for f in futures]\n",
    "    \n",
    "    \n",
    "impact_results = parallel(\n",
    "    prompt,\n",
    "    \n",
    "    \"\"\"\n",
    "    ChatGPT는 OpenAI가 개발한 대형 언어 모델입니다.\",\n",
    "    \"AI 기술은 산업 전반에 혁신을 불러오고 있습니다.\",\n",
    "    \"기후 변화는 글로벌 이슈 중 하나입니다.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    ")\n",
    "\n",
    "for result in impact_results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import AsyncGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from utils import llm_call_async, llm_call\n",
    "import asyncio\n",
    "\n",
    "async def run_llm_parallel(prompt_details):\n",
    "    tasks = [llm_call_async(prompt['user_prompt'], prompt['model']) for prompt in prompt_details]\n",
    "    responses = []\n",
    "    \n",
    "    for task in asyncio.as_completed(tasks):\n",
    "        result = await task\n",
    "        print(\"LLM 응답 완료:\", result)\n",
    "        responses.append(result)\n",
    "    \n",
    "    return responses\n",
    "\n",
    "\n",
    "\n",
    "async def main():\n",
    "    question = (\"아래 문장을 자연스러운 한국어로 번역해줘:\\n\"\n",
    "                \"\\\"Do what you can, with what you have, where you are.\\\" — Theodore Roosevelt\")\n",
    "    \n",
    "    parallel_prompt_details = [\n",
    "        {\"user_prompt\": question, \"model\": \"gemma2-9b-it\"},\n",
    "        {\"user_prompt\": question, \"model\": \"llama3-70b-8192\"},\n",
    "        {\"user_prompt\": question, \"model\": \"deepseek-r1-distill-llama-70b-specdec\"},\n",
    "    ]\n",
    "    \n",
    "    responses = await run_llm_parallel(parallel_prompt_details)\n",
    "    \n",
    "    aggregator_prompt = (\"다음은 여러 개의 AI 모델이 사용자 질문에 대해 생성한 응답입니다.\\n\"\n",
    "                         \"당신의 역할은 이 응답들을 모두 종합하여 최종 답변을 제공하는 것입니다.\\n\"\n",
    "                         \"일부 응답이 부정확하거나 편향될 수 있으므로, 신뢰성과 정확성을 갖춘 응답을 생성하는 것이 중요합니다.\\n\\n\"\n",
    "                         \"사용자 질문:\\n\"\n",
    "                         f\"{question}\\n\\n\"\n",
    "                         \"모델 응답들:\")\n",
    "    \n",
    "    for i in range(len(parallel_prompt_details)):\n",
    "        aggregator_prompt += f\"\\n{i+1}. 모델 응답: {responses[i]}\\n\"\n",
    "    \n",
    "    print(\"---------------------------종합 프롬프트:-----------------------\\n\", aggregator_prompt)\n",
    "    final_response = await llm_call_async(aggregator_prompt, model=\"llama3-70b-8192\")\n",
    "    print(\"---------------------------최종 종합 응답:-----------------------\\n\", final_response)\n",
    "\n",
    "# 비동기 main 함수 실행\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "동기적으로 하나씩 순차적으로 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import llm_call\n",
    "\n",
    "user_input = '''업무적 피로감으로 인해 편하게힐링 할 수 있는 곳으로 여행을 가고 싶어 \n",
    "                여행지를 2가지만 추천해줘 추천 여행지, 여행 코스를 알려주고 그 이유도 알려줘\n",
    "                나는 여러 관광지를 돌아다는것 보다는 경치좋고 특색있는 명소를 찾아다니는것을 좋아해\n",
    "                아래 포맷대로 작성해줘 \n",
    "                \n",
    "                추천 여행지 :\n",
    "                1. OO\n",
    "                \n",
    "                추천 코스:\n",
    "                OO => OO => OO\n",
    "                \n",
    "                추천 이유:\n",
    "                OO\n",
    "                \n",
    "                \n",
    "                '''\n",
    "\n",
    "prompt = [\n",
    "    \n",
    "    {'user' : user_input, 'model' : 'gemma2-9b-it'},\n",
    "    {'user' : user_input, 'model' : 'llama3-70b-8192'},\n",
    "    {'user' : user_input, 'model' : 'deepseek-r1-distill-llama-70b-specdec'}\n",
    "    \n",
    "]\n",
    "\n",
    "for prom in prompt:\n",
    "    \n",
    "    print(f'{prom[\"model\"]}')\n",
    "    response = llm_call(prom['user'], prom['model'])\n",
    "    \n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asyncio.create_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gemma2-9b-it\n",
      "llama3-70b-8192\n",
      "deepseek-r1-distill-llama-70b-specdec\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(prompt)):\n",
    "    print(f'{prompt[i][\"model\"]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "<coroutine object as_completed.<locals>._wait_for_one at 0x000002AF4C004A00>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 85\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28mprint\u001b[39m(result)\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# 비동기 main 함수 실행\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m main(user_input)\n",
      "Cell \u001b[1;32mIn[40], line 60\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(user_input)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmain\u001b[39m(user_input):\n\u001b[0;32m     52\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     53\u001b[0m     \n\u001b[0;32m     54\u001b[0m     {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m'\u001b[39m : user_input, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgemma2-9b-it\u001b[39m\u001b[38;5;124m'\u001b[39m},\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \n\u001b[0;32m     58\u001b[0m     ]\n\u001b[1;32m---> 60\u001b[0m     responses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m parallel(prompt)\n\u001b[0;32m     62\u001b[0m     ensemble_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'''\u001b[39m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;124m입력된 텍스트은 사용자의 요청에 따라 출력한 여러가지 결과입니다. \u001b[39m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;124m이 결과들을 종합하고 요약하여 사용자의 질문에 맞는 최종 결과를 출력해주세요.\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;124m모델별 응답:\u001b[39m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124m'''\u001b[39m\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m responses:\n",
      "Cell \u001b[1;32mIn[40], line 43\u001b[0m, in \u001b[0;36mparallel\u001b[1;34m(prompt)\u001b[0m\n\u001b[0;32m     41\u001b[0m responses \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mas_completed(task_map):\n\u001b[1;32m---> 43\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mtask_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     44\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m task\n\u001b[0;32m     45\u001b[0m     responses\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m: model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m'\u001b[39m: response})\n",
      "\u001b[1;31mKeyError\u001b[0m: <coroutine object as_completed.<locals>._wait_for_one at 0x000002AF4C004A00>"
     ]
    }
   ],
   "source": [
    "# 비동기 함수를 사용하여 여러 모델에 대한 요청을 병렬로 처리하는 예제입니다.\n",
    "from utils import llm_call_async\n",
    "import asyncio\n",
    "\n",
    "user_input = '''\n",
    "업무적 피로감으로 인해 편하게힐링 할 수 있는 곳으로 여행을 가고 싶어 \n",
    "여행지를 2가지만 추천해줘 추천 여행지, 여행 코스를 알려주고 그 이유도 알려줘\n",
    "나는 여러 관광지를 돌아다는것 보다는 경치좋고 특색있는 명소를 찾아다니는것을 좋아해\n",
    "아래 포맷대로 작성해줘 \n",
    "\n",
    "추천 여행지 :\n",
    "1. OO\n",
    "\n",
    "추천 코스:\n",
    "OO => OO => OO\n",
    "\n",
    "추천 이유:\n",
    "OO\n",
    "'''\n",
    "\n",
    "\n",
    "async def parallel(prompt):\n",
    "    tasks = []\n",
    "    for prom in prompt:\n",
    "        tasks.append(llm_call_async(prom['user'], prom['model']))\n",
    "        \n",
    "    responses = []\n",
    "    for task in asyncio.as_completed(tasks):\n",
    "        result = await task\n",
    "        responses.append(result)\n",
    "    \n",
    "    return responses\n",
    "\n",
    "\n",
    "async def main(user_input):\n",
    "    \n",
    "    prompt = [\n",
    "    \n",
    "    {'user' : user_input, 'model' : 'gemma2-9b-it'},\n",
    "    {'user' : user_input, 'model' : 'llama3-70b-8192'},\n",
    "    {'user' : user_input, 'model' : 'deepseek-r1-distill-llama-70b-specdec'}\n",
    "    \n",
    "    ]\n",
    "    \n",
    "    responses = await parallel(prompt)\n",
    "    \n",
    "    ensemble_prompt = f'''\n",
    "입력된 텍스트은 사용자의 요청에 따라 출력한 여러가지 결과입니다. \n",
    "이 결과들을 종합하고 요약하여 사용자의 질문에 맞는 최종 결과를 출력해주세요.\n",
    "\n",
    "사용자 질문:\n",
    "    {user_input}\n",
    "\n",
    "모델별 응답:\n",
    "'''\n",
    "                    \n",
    "    for i in range(len(prompt)):\n",
    "        ensemble_prompt += f'model :{prompt[i][\"model\"]}\\n모델 응답: {responses[i]}\\n'\n",
    "        \n",
    "    result = await llm_call_async(ensemble_prompt, model=\"gemma2-9b-it\")\n",
    "    \n",
    "    print(\"=========================프롬프트==========================\\n\")\n",
    "    print(ensemble_prompt)\n",
    "    \n",
    "    print(\"=========================출력 결과=========================\")\n",
    "    print(result)\n",
    "\n",
    "# 비동기 main 함수 실행\n",
    "await main(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(prompt)):\n",
    "    ensemble_prompt += f'model :{prompt[i][\"model\"]}\\n모델 응답: {responses[i]}\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''.join([f'model :{prompt[i][\"model\"]}\\n모델 응답: {responses[i]}\\n' for i in range(len(prompt))])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
